{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9056f590",
   "metadata": {},
   "source": [
    "# **Reinforcement Learning: An Introduction** - Chapter 9\n",
    "Thomas Hopkins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55adc878",
   "metadata": {},
   "source": [
    "## **Exercise 9.1**\n",
    "I do not think that the eligibility trace method would perform as well as the model planning method. This is because the trace for each state would fall off for the states that were many steps away from the goal. Furthermore, some of the states involved in the eligibility trace update would point to the state that followed it most recently. This problem does not occur in the model planning method because each transition encountered is stored as part of the model and randomly sampled during simulated learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8d87c",
   "metadata": {},
   "source": [
    "## **Exercise 9.2**\n",
    "The DynaQ+ agent likely performed better in both phases due to its exploration. It prioritized its exploration based on its model of the environment while the others explored randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a9bca",
   "metadata": {},
   "source": [
    "## **Exercise 9.3**\n",
    "The difference between DynaQ+ and DynaQ narrowed because the DynaQ+ agent spends some of its computation exploring states that have not been encountered in a while. This allows the other DynaQ agent to \"catch up\" in cumulative reward by not exploring nearly as much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb30cfa",
   "metadata": {},
   "source": [
    "## **Exercise 9.4**\n",
    "It seems like the DynaQ+ agent with the exploration mechanism performed during action selection performs better than the mechanism that alters rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7220ba1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dynaq_plus (generic function with 3 methods)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Base.Iterators\n",
    "using Random\n",
    "using Plots\n",
    "gr()\n",
    "\n",
    "STATES = collect(product(1:6, 1:9))\n",
    "GRID = zeros(6, 9)\n",
    "GRID[4, 2:9] .= 1\n",
    "ACTIONS = [(0, -1), (-1, 0), (0, 1), (1, 0)] # (left, up, right, down)\n",
    "START = (6, 4)\n",
    "GOAL = (1, 9)\n",
    "    \n",
    "\n",
    "function select_action(Q_values, state, n_vals; kappa = 0.1, method = false, greedy = false)\n",
    "    if !greedy\n",
    "        if rand() < 0.1\n",
    "            action = rand(ACTIONS)\n",
    "            return findfirst(x -> all(x .== action), ACTIONS), action\n",
    "        end\n",
    "    end\n",
    "    values = Q_values[state...]\n",
    "    if method\n",
    "        values .+= (kappa * sqrt.(n_vals[state...]))\n",
    "    end\n",
    "    a = argmax(values)\n",
    "    return a, ACTIONS[a]\n",
    "end\n",
    "\n",
    "function step(state, action)\n",
    "    new_state = [state[1] + action[1], state[2] + action[2]]\n",
    "    if new_state[1] <= 0\n",
    "        new_state[1] = 1\n",
    "    elseif new_state[1] > 6\n",
    "        new_state[1] = 6\n",
    "    end\n",
    "    if new_state[2] <= 0\n",
    "        new_state[2] = 1\n",
    "    elseif new_state[2] > 9\n",
    "        new_state[2] = 9\n",
    "    end\n",
    "    if GRID[new_state...] == 1\n",
    "        new_state = state\n",
    "    else\n",
    "        new_state = (new_state[1], new_state[2])\n",
    "    end\n",
    "    reward = 0.0\n",
    "    terminated = false\n",
    "    if new_state == GOAL\n",
    "        reward = 1.0\n",
    "        terminated = true\n",
    "    end\n",
    "    return reward, new_state, terminated\n",
    "end\n",
    "\n",
    "function dynaq_plus(Q_values, model, n_vals; n=5, num_time_steps = 50000, kappa = 0.5, alpha = 0.1, gamma = 0.95, method = false)\n",
    "    cumulative_reward = 0.0\n",
    "    state = START\n",
    "    a, action = select_action(Q_values, state, n_vals; kappa = kappa, method = method, greedy = false)\n",
    "    terminated = false\n",
    "    for e = 1:num_time_steps\n",
    "        a, action = select_action(Q_values, state, n_vals; kappa = kappa, method = method, greedy = false)\n",
    "        reward, new_state, terminated = step(state, action)\n",
    "        na, new_action = select_action(Q_values, new_state, n_vals; greedy = true)\n",
    "        if !method\n",
    "            Q_values[state...][a] += alpha * ((reward + kappa * sqrt.(n_vals[state...][a])) + gamma * Q_values[new_state...][na] - Q_values[state...][a])\n",
    "        else\n",
    "            Q_values[state...][a] += alpha * (reward + gamma * Q_values[new_state...][na] - Q_values[state...][a])\n",
    "        end\n",
    "        for i = 1:6, j = 1:9\n",
    "            n_vals[i, j] .+= 1\n",
    "        end\n",
    "        n_vals[state...][a] = 0\n",
    "        model[state...][a] = (new_state, reward)\n",
    "        state = new_state\n",
    "        if terminated\n",
    "            state = START\n",
    "        end\n",
    "        cumulative_reward += reward\n",
    "        for i = 1:n\n",
    "            S = findall(x -> any([!ismissing(x[i]) for i = 1:4]), model)\n",
    "            s = rand(S)\n",
    "            A = findall(x -> !ismissing(x), model[s])\n",
    "            a = rand(A)\n",
    "            sp, r = model[s][a]\n",
    "            ap, _ = select_action(Q_values, sp, n_vals; greedy = true)\n",
    "            Q_values[s][a] += alpha * (reward + gamma * Q_values[sp...][ap] - Q_values[s][a])\n",
    "        end\n",
    "    end\n",
    "    return Q_values, cumulative_reward\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4218f58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[17.04668001194853, 17.112144798924962, 18.183839071035756, 17.680571550212512] [17.05680452976106, 18.251288132229085, 19.673205335867912, 18.848569942891046] … [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0]; [17.62706195519849, 17.097717715797128, 19.043217637726272, 17.754445637428045] [17.721813245203823, 17.88857767031447, 20.43074727026122, 18.695405340047767] … [18.554805277684135, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0]; … ; [16.496064306291835, 17.063876045394597, 15.834178562978641, 15.87403683422662] [16.45509899899592, 15.953579727633313, 15.457256724639448, 15.346112316512855] … [12.997948983920647, 12.45500518827363, 11.912014102098347, 11.940747391556904] [12.358036841680798, 0.0, 0.0, 0.0]; [15.97848437102069, 16.55119000909181, 15.45968936026643, 15.972502041430257] [15.963520503637955, 15.972325262126615, 14.822553931684038, 15.421811825057414] … [12.382676646065885, 12.419725584130235, 11.523268744527625, 12.038255541029246] [11.952089961904017, 11.88620721740693, 0.0, 11.55840621738445]], 0.0)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = [[0.0 for a in ACTIONS] for i = 1:6, j = 1:9]\n",
    "model = [Any[missing for a in ACTIONS] for i = 1:6, j = 1:9]\n",
    "n_vals = [[0, 0, 0, 0] for i = 1:6, j = 1:9]\n",
    "Q, reward_original = dynaq_plus(q_values, model, n_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2020f955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[605.0298209986272, 605.3467846460104, 575.3426952295258, 635.5793597757557] [604.7316509499418, 575.3174646013858, 546.6665554141003, 604.9234572978479] … [451.5653192399962, 433.89089971122, 1.7493640022644337e-9, 447.64837249492007] [0.0, 0.0, 0.0, 0.0]; [636.2610092579076, 604.8055618773469, 604.6934931088492, 667.2147955040028] [635.1922103535043, 575.4519327856597, 574.4559636062952, 634.901907831363] … [468.31844382016175, 431.9696464826603, 426.3868292499176, 468.08117369537774] [446.433456447583, 1.1527725762606654e-7, 427.45910824984463, 446.4906147416382]; … ; [761.4160740929423, 754.8397761192113, 768.8041934214067, 766.6382256571759] [773.3103016503813, 767.9723714121699, 779.3773388596063, 766.7809956862867] … [701.2744223139007, 672.40960911291, 643.3043842893677, 670.5323125226782] [672.1204920648514, 641.9606197846043, 642.2973142211533, 636.7442401803871]; [749.6928523406486, 759.1738999205086, 758.1130630254507, 749.1436377057173] [771.5042787885719, 769.6697365349261, 770.9825694440248, 763.440668457792] … [701.3354675955009, 671.4120379176939, 636.5345255971066, 666.5805399318713] [666.9044447760939, 641.1451651715622, 637.5137230070223, 636.773819936747]], 13.0)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = [[0.0 for a in ACTIONS] for i = 1:6, j = 1:9]\n",
    "model = [Any[missing for a in ACTIONS] for i = 1:6, j = 1:9]\n",
    "n_vals = [[0, 0, 0, 0] for i = 1:6, j = 1:9]\n",
    "Q, reward_new = dynaq_plus(q_values, model, n_vals; method = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3cb5d3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bcfabb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e93fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
