{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ccf9f7",
   "metadata": {},
   "source": [
    "# **Reinforcement Learning: An Introduction** - Chapter 6 \n",
    "Thomas Hopkins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17aacb9",
   "metadata": {},
   "source": [
    "## **Exercise 6.1**\n",
    "Given the scenario offered in the book, moving to a new building for work but entering the highway at the same place on your way home gives a big advantage for TD methods compared to Monte Carlo methods. This is because with a large amount of experience driving home from the old building, the estimate for the time it takes to get home from the highway entrance is very accurate. This data is utilized by TD methods but *not* by Monte Carlo methods. This is one way in which boostrapping is powerful. It can rapidly adapt to changes when new states are encountered. This kind of thing happens even in the original scenario, coming home from the old building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156fc6f",
   "metadata": {},
   "source": [
    "## **Exercise 6.2**\n",
    "This does not tell much about what happened in the first episode other than that the episode terminated on the left. The update to the value occured as\n",
    "$$V(A) = 0.5 + 0.1(0.0 + (1.0)(0.0) - 0.5) = 0.5 - 0.05 = 0.45$$ \n",
    "This is because the value of terminal states are always $0$, by definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61fc02",
   "metadata": {},
   "source": [
    "## **Exercise 6.3**\n",
    "Given only 100 episodes, I do not think that changing the value of $\\alpha$ would lower the RMS error. Given a larger, but still finite, number of episodes, a lower $\\alpha$ would be better since it will take smaller steps. The problem with a limit of 100 episodes to learn from is that a smaller $\\alpha$ may not converge by the time that limit is reached. One could get the best of both worlds by decreasing the learning rate as the number of episodes increases, however, finding the correct schedule for decreasing the learning rate is its own problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af1c5b",
   "metadata": {},
   "source": [
    "## **Exercise 6.4**\n",
    "As learning continues, this will always occur. The severity of this issue depends on the size of the learning rate, $\\alpha$. TD methods continue to update the states using this learning rate, so fluctuations can occur and errors can propagate. This could be an issue of how the approximate value function was initialized since it could have introduced bias into the function that would be difficult to unlearn. For instance, $V(A) = 0.5$ is quite far from its true value while $V(C) = 0.5$ is spot on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258c709",
   "metadata": {},
   "source": [
    "## **Exercise 6.5**\n",
    "One way is by reasoning that $V(C) = 0.5$ and then we can compute the values of all of the other states by solving a system of linear equations. The other way is by using dynamic programming methods. Solving the linear system is probably easier.\n",
    "\n",
    "The linear system would be\n",
    "$$V(C) = \\frac{1}{2}$$\n",
    "$$V(A) = \\frac{V(B)}{2}$$\n",
    "$$V(B) = \\frac{V(A) + V(C)}{2}$$\n",
    "$$V(D) = \\frac{V(C) + V(E)}{2}$$\n",
    "$$V(E) = \\frac{1 + V(D)}{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a23c75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
