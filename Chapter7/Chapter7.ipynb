{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5156598",
   "metadata": {},
   "source": [
    "# **Reinforcement Learning: An Introduction** - Chapter 7\n",
    "Thomas Hopkins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0caea2",
   "metadata": {},
   "source": [
    "## **Exercise 7.1**\n",
    "The larger random walk task was most likely used because it would allow for larger $n$ steps to be taken and tested. A smaller walk would most likely shift the value of $n$ down. The change from the left-side end state from $0$ to $-1$ would cause larger $n$ to be more useful since a reward other than $0$ is computed along the $n$-step return if it reaches that terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1912169",
   "metadata": {},
   "source": [
    "## **Exercise 7.2**\n",
    "On-line methods most likely worked better than off-line methods because better estimates for values were used earlier in the learning process. Waiting until the end of the episode delays the ability to use the most up-to-date information. On the other hand, on-line methods allow this new information to be used immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8524ff7f",
   "metadata": {},
   "source": [
    "## **Exercise 7.3**\n",
    "The only explanation I can think of is that 3-step returns are particularly noisy for this choice of initialization and number of states in the random walk. So, using a higher learning rate, $\\alpha$, causes the error to increase sharply over the first 10 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128347b",
   "metadata": {},
   "source": [
    "## **Exercise 7.4**\n",
    "The equation for the weighting at time $t$ given by $\\lambda$ and its half-life $\\tau_{\\lambda}$ is\n",
    "$$(1 - \\lambda)\\lambda^t = (1 - \\lambda)(\\frac{1}{2})^{t/\\tau_{\\lambda}}$$\n",
    "$$\\Rightarrow \\lambda^t = (\\frac{1}{2})^{t/\\tau_{\\lambda}}$$\n",
    "$$\\Rightarrow t\\ln(\\lambda) = \\frac{t}{\\tau_{\\lambda}}\\ln( \\frac{1}{2})$$\n",
    "$$\\Rightarrow \\tau_{\\lambda} = -\\frac{\\ln(2)}{\\ln(\\lambda)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199bc694",
   "metadata": {},
   "source": [
    "## **Exercise 7.5**\n",
    "$$\\Delta V_t(s_t) = \\alpha \\delta_t e_t(s)$$\n",
    "$$ = \\alpha (r_{t+1} + \\gamma V_t(s_{t+1}) - V_{t-1}(s_t)) (\\sum_{k=0}^t (\\gamma \\lambda)^{t-k} I_{ss_k})$$\n",
    "from the derviation on page 177, we have\n",
    "$$ = \\alpha [ R^{\\lambda}_t - V_{t-1}(s_t)]$$\n",
    "This method would benefit from being able to update values during the episode. As described before, this helps in using recent information sooner. In this case, however, you will only get the immediate information of the return on-line. It might be worse since the other on-line method would be able to use the updated values of each state in its update as well.\n",
    "\n",
    "An experiment to assess the relative merits would be to create two environments. One would have large immediate rewards and the other would have small immediate rewards. Run both algorithms and compare the results across different values of $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8897b5ed",
   "metadata": {},
   "source": [
    "## **Exercise 7.6**\n",
    "For accumuulating traces, the update is:\n",
    "$$e_t(s, a) = \\begin{cases}\n",
    "\\gamma \\lambda e_{t-1}(s) + 1 & s_t = s, a_t = a \\\\\n",
    "\\gamma \\lambda e_{t-1}(s) & s_t = s, a_t = a\n",
    "\\end{cases}$$\n",
    "With two $a = wrong$ then one $a = right$, we have\n",
    "$$e_1(s, wrong) = 1$$\n",
    "$$e_1(s, right) = 0$$\n",
    "$$e_2(s, wrong) = \\gamma \\lambda + 1$$\n",
    "$$e_2(s, right) = 0$$\n",
    "$$e_3(s, wrong) = \\gamma \\lambda (\\gamma \\lambda + 1)$$\n",
    "$$e_3(s, right) = 1$$\n",
    "Assuming $\\gamma = 1$ we have\n",
    "$$\\lambda ( \\lambda + 1 ) > 1$$\n",
    "$$\\Rightarrow \\lambda^2 + \\lambda - 1 > 0$$\n",
    "$$\\Rightarrow \\lambda > -\\frac{1 - \\sqrt{5}}{2}$$\n",
    "If $\\lambda > 0.618$, then $wrong$ will have a larger eligibility trace than $right$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77aa285",
   "metadata": {},
   "source": [
    "## **Exercise 7.7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "783084e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct StateActionPair\n",
    "    state::Int\n",
    "    action::String\n",
    "    trace::Float64\n",
    "    value::Float64\n",
    "end\n",
    "\n",
    "# 1 is right, 2 is wrong for actions\n",
    "function sarsa_lambda(num_states::Int; num_episodes = 100, alpha = 0.1,\n",
    "                      gamma = 1.0, lambda = 0.9, epsilon = 0.5, replacing_trace = false)\n",
    "    env = [[StateActionPair(i, \"right\", 0.0, 0.0),\n",
    "            StateActionPair(i, \"wrong\", 0.0, 0.0)]\n",
    "           for i = 1:num_states]\n",
    "    \n",
    "    for e = 1:num_episodes\n",
    "        s = env[1]\n",
    "        sap = rand(s)\n",
    "        terminated = false\n",
    "        while !terminated\n",
    "            new_s = s\n",
    "            reward = 0.0\n",
    "            if sap.action == \"right\"\n",
    "                if sap.state == num_states\n",
    "                    terminated = true\n",
    "                    new_s = \n",
    "                    reward = 1.0\n",
    "                else\n",
    "                    new_s = env[sap.state + 1]\n",
    "                end\n",
    "            end\n",
    "            if terminated\n",
    "                new_sap = StateActionPair(num_states + 1, \"none\", 0.0, 0.0)\n",
    "            else\n",
    "                new_sap = new_s[argmax([new_s[1].value, new_s[2].value])]\n",
    "                if rand() < epsilon\n",
    "                    new_sap = rand(new_s)\n",
    "                end\n",
    "            end\n",
    "            delta = reward + gamma * new_sap.value - sap.value\n",
    "            if replacing_trace\n",
    "                sap.trace = 1\n",
    "            else\n",
    "                sap.trace = sap.trace + 1\n",
    "            end\n",
    "            for i = 1:num_states\n",
    "                for j = 1:2\n",
    "                    env[i][j].value = env[i][j].value + alpha * delta * env[i][j].trace\n",
    "                    env[i][j].trace = gamma * lambda * env[i][j].trace\n",
    "                end\n",
    "            end\n",
    "            sap = new_sap\n",
    "        end\n",
    "    end\n",
    "    return env\n",
    "end\n",
    "\n",
    "function display_values(env)\n",
    "    println(\"Values for each state\")\n",
    "    println(\"---------------------\")\n",
    "    for s in env\n",
    "        println(\"State: $(s[1].state)\")\n",
    "        println(\"Right: $(s[1].value)\")\n",
    "        println(\"Wrong: $(s[2].value)\")\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "426e2e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating Trace\n",
      "Values for each state\n",
      "---------------------\n",
      "State: 1\n",
      "Right: -2.1463301237429342e9\n",
      "Wrong: 3.1251120520893908e9\n",
      "State: 2\n",
      "Right: -1.9868580042852964e9\n",
      "Wrong: 5.301282924569049e8\n",
      "State: 3\n",
      "Right: -1.756881357948165e8\n",
      "Wrong: 6.227890555493832e8\n",
      "State: 4\n",
      "Right: -6.194646549525633e8\n",
      "Wrong: -51971.52634669248\n",
      "State: 5\n",
      "Right: -1.117016311869059e9\n",
      "Wrong: 2.1175084255286857e7\n",
      "\n",
      "Replacing Trace\n",
      "Values for each state\n",
      "---------------------\n",
      "State: 1\n",
      "Right: 0.9674348569717787\n",
      "Wrong: 0.9039216250956357\n",
      "State: 2\n",
      "Right: 0.9840439068929528\n",
      "Wrong: 0.0\n",
      "State: 3\n",
      "Right: 0.9994466628362726\n",
      "Wrong: 0.2238675776586064\n",
      "State: 4\n",
      "Right: 1.0042490635732428\n",
      "Wrong: 0.0\n",
      "State: 5\n",
      "Right: 1.0074991628992664\n",
      "Wrong: 0.0\n"
     ]
    }
   ],
   "source": [
    "using Random\n",
    "\n",
    "Random.seed!(32)\n",
    "N = 5\n",
    "episodes = 10\n",
    "alpha = 0.9\n",
    "epsilon = 0.1\n",
    "accumulate_env = sarsa_lambda(N; num_episodes = episodes, alpha = alpha, epsilon = epsilon)\n",
    "replacing_env = sarsa_lambda(N; num_episodes = episodes, alpha = alpha, epsilon = epsilon, replacing_trace = true)\n",
    "println(\"Accumulating Trace\")\n",
    "display_values(accumulate_env)\n",
    "println(\"\\nReplacing Trace\")\n",
    "display_values(replacing_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a99ba",
   "metadata": {},
   "source": [
    "By trying different values of $\\alpha$, we see that replacing traces allow for a much larger value while accumulating traces are unusable at larger values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad0d9a",
   "metadata": {},
   "source": [
    "## **Exercise 7.8**\n",
    "The backup diagram would be the same as Figure 7.10 since the weighting does not account for repeated states. The change would be in how you combine the different weights to get the real trace for a particular state-action pair. This would have to be a maximum of 1 for replacing traces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2715a453",
   "metadata": {},
   "source": [
    "## **Exercise 7.9**\n",
    "```\n",
    "Initialize V(s) arbitrarily and e(s) = 0, for all s in S\n",
    "Repeat (for each episode):\n",
    "    Initialize s\n",
    "    Repeat (for each step of episode):\n",
    "        a <- action given by pi for s\n",
    "        Take action a, observe reward, r, and next state, s'\n",
    "        delta <- r + gamma * V(s') - V(s)\n",
    "        e(s) <- e(s) + 1\n",
    "        For all s:\n",
    "            if e(s) > c:\n",
    "                V(s) <- V(s) + alpha * delta * e(s)\n",
    "            e(s) <- gamma * lambda * e(s)\n",
    "        s <- s'\n",
    "    until s is terminal\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e51a1a",
   "metadata": {},
   "source": [
    "## **Exercise 7.10**\n",
    "This proof is very similar to that in Section 7.4 and Exercise 7.5 above except that we now have a variable $\\lambda$ term that depends on $t$. The only change to the proof is the addition of a subscript to $\\lambda$ indicating which time step the particular $\\lambda_t$ is referring to."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
