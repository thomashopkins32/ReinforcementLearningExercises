{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d99128",
   "metadata": {},
   "source": [
    "# **Reinforcement Learning: An Introduction** - Chapter 8\n",
    "Thomas Hopkins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ca9da",
   "metadata": {},
   "source": [
    "## **Exercise 8.1**\n",
    "Table-lookup TD($\\lambda$) is a special case of general TD($\\lambda$) where each component of $\\vec \\theta_t$ represents a single state $s \\in S$. The term $\\nabla_{\\vec \\theta_t} V_t(s_t)$ becomes a 1-hot vector (zero vector with a 1 in a single component) that represents the state being updated. This method simply updates all of the states in one sweep using vector addition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c7479",
   "metadata": {},
   "source": [
    "## **Exercise 8.2**\n",
    "Similar to the previous exercise, the state aggregation method has one component of the parameter vector per group of states. The gradient vector is simply a 1-hot vector representing which group of states, or component of the parameter vector, should be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17494afe",
   "metadata": {},
   "source": [
    "## **Exercise 8.3**\n",
    "Let\n",
    "$$\\Delta \\vec \\theta_t = \\alpha [R_t^{\\lambda} - V_t(s_t)] \\nabla_{\\vec \\theta_t} V_t(s_t)$$\n",
    "be the update for the on-line method (8.4). The off-line method would then have\n",
    "$$\\Delta \\vec \\theta = \\alpha \\sum_{t=0}^{T-1} [R_t^{\\lambda} - V_t(s_t)] \\nabla_{\\vec \\theta_t} V_t(s_t)$$\n",
    "That is, the gradients need to be accumulated throughout the episode, then updated after the episode terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bbd2b7",
   "metadata": {},
   "source": [
    "## **Exercise 8.4**\n",
    "The off-line version of the backward view is\n",
    "$$\\vec \\theta = \\vec \\theta + \\alpha \\sum_{t=0}^{T-1} \\delta_t \\vec e_t$$\n",
    "where\n",
    "$$\\Delta \\vec \\theta = \\alpha \\sum_{t=0}^{T-1} \\delta_t \\vec e_t$$\n",
    "As seen in the previous chapter, the forward and backward views are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4352084",
   "metadata": {},
   "source": [
    "## **Exercise 8.5**\n",
    "We can reproduce the tabular case of reinforcemnet learning using the linear framework by having a single feature per state. Then when that state is encountered, only that feature is turned on (given a value of 1) while all of the other features are turned off (given a value of 0). For most problems, this is not possible since there are far too many possible states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a32ecd",
   "metadata": {},
   "source": [
    "## **Exercise 8.6**\n",
    "Similar to the previous exercise, we would have a single feature per group of states. As long as the state aggregation method is known beforehand, we can assign a single feature to a group and turn it on when a state in that group is encountered. All of the other features are turned off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4fb9a7",
   "metadata": {},
   "source": [
    "## **Exercise 8.7**\n",
    "A vertical or horizontal striped tiling would make sense here. The generalization occurs along the stripe while the discrimation occurs across it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b192f923",
   "metadata": {},
   "source": [
    "## **Exercise 8.8**\n",
    "The actor-critic control method can be extended to use function approximation by allowing the critic to estimate the value of a state using its own parameter vector while the actor uses its own parameter vector to estimate the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead738d",
   "metadata": {},
   "source": [
    "## **Exercise 8.9**\n",
    "Optimal weights are the zero vector. The TD(0) method diverges in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4fcdab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting weights: [1.0, 1.0, 1.0, 1.0, 1.0, 10.0, 1.0]\n",
      "After 100 episodes: [1.720000000000135, 1.7200000000001403, 1.720000000000135, 1.7200000000001394, 1.720000000000135, 6.880000000000546, -3.440000000000271]\n"
     ]
    }
   ],
   "source": [
    "using Random\n",
    "\n",
    "Random.seed!(32)\n",
    "weights = [1.0 for i = 1:7]\n",
    "weights[6] = 10.0\n",
    "alpha = 0.01 \n",
    "gamma = 0.99\n",
    "\n",
    "println(\"Starting weights: $weights\")\n",
    "for e = 1:5000\n",
    "    # start state\n",
    "    s = rand(1:5)\n",
    "    q_s = weights[7] + 2 * weights[s]\n",
    "    # next state is always 6\n",
    "    sp = 6\n",
    "    q_sp = 2 * weights[7] + weights[sp]\n",
    "    delta = gamma * q_sp - q_s\n",
    "    # derivative with respect to weight 7 is 1\n",
    "    weights[7] = weights[7] + alpha * delta * 1\n",
    "    # derivative with sepect to weight s is 2 \n",
    "    weights[s] = weights[s] + alpha * delta * 2\n",
    "    # with probability 0.01 the episode ends\n",
    "    # otherwise we repeat state 6\n",
    "    while rand() > 0.01\n",
    "        q = 2 * weights[7] + weights[6]\n",
    "        delta = gamma * q - q\n",
    "        weights[7] = weights[7] + alpha * delta * 2\n",
    "        weights[6] = weights[6] + alpha * delta * 1\n",
    "    end\n",
    "    q = 2 * weights[7] + weights[6]\n",
    "    delta = -q\n",
    "    weights[7] = weights[7] + alpha * delta * 2\n",
    "    weights[6] = weights[6] + alpha * delta * 1\n",
    "end\n",
    "println(\"After 100 episodes: $weights\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
